# Q-Pensieve: Boosting Sample Efficiency of Multi-Objective RL Through Memory Sharing of Q-Snapshots

![](https://i.imgur.com/7Zuv6Jw.png)

**Q-Pensieve: Boosting Sample Efficiency of Multi-Objective RL Through Memory Sharing of Q-Snapshots**

Wei Hung\*, Bo Kai Huang\*, [Ping-Chun Hsieh](https://pinghsieh.github.io/), Xi Liu

*The Eleventh Conference on International Conference on Learning Representations (ICLR 2023)*

[[Paper]](https://openreview.net/pdf?id=AwWaBXLIJE)

## Code Structure

```
Q-Pensieve/
 ├── environments/ --- the test environment includes continuous Deep Sea Treasure (DST), multi-objective continuous LunarLander and multi-objective MuJoCo
 ├── 3pref_table.npy 4pref_table.npy 5pref_table.npy  --- testing preference generated by leveraging the Dirichlet distribution
 ├── agent.py --- the trainning agent of Q-Pensieve
 ├── test.py ---  main execution file for testing model with Q-Pensieve algorithms
 ├── compute_hv.py --- the execution file to caculate the hypervolume with trained model
 ├── main.py ---  main execution file for Q-Pensieve algorithms
 ├── model.py --- the model of Q-Pensieve
 ├── base.py --- the structure of Q replay buffer and trajectory replay buffer
 └── utils.py --- utility functions
 
 

```

## Requirements

- Python version: tested in Python 3.9.12

- Operating system: tested in Ubuntu 20.04

- Pytorch version: 1.3.1

Install other required packages:

```
pip install -r requirements.txt
```

## Basic usage

### Training

Run the main file with python:

```shell
python main.py --env_id "MO_hopper-v0" --seed 1 --prefer 4 --buf_num 4 --q_freq 1000
```
config: 
- prefer: The size of preference set.
- buf_num: The size of Q replay buffer.
- q_freq: The update interval of Q replay buffer.


### Testing

The results are saved under the folder:

```
logs/{env_id}/MOSAC-set{prefer}-buf{buf_num}-seed{seed}_freq{q_freq}/model
logs/{env_id}/MOSAC-set{prefer}-buf{buf_num}-seed{seed}_freq{q_freq}/summary
```

Models are in pth format, and results are saved in npy format.
And then we can execute the main test file with python:

```
python test.py --env_id "MO_hopper-v0" --seed 1 --set_num 4 --buf_num 4 --q_freq 1000 --model_id 15 --ref_point [0, -300]
```
config: 
- set_num: The size of preference set.
- buf_num: The size of Q replay buffer.
- q_freq: The update interval of Q replay buffer.
- ref_point: The refference point to caculate hypervolume
- model_id: The model you want to train is in {model_saved_step} * {model_id} steps.

*Explanation --- model_saved_step is the interval between model saved.

## Refference
[rltorch](https://github.com/toshikwa/rltorch) -  a simple framework for reinforcement learning in PyTorch.

## Citation

If you think our program is helpful, you can cite our work:

```
@inproceedings{
hung2023qpensieve,
title={Q-Pensieve: Boosting Sample Efficiency of Multi-Objective {RL} Through Memory Sharing of Q-Snapshots},
author={Wei Hung and Bo Kai Huang and Ping-Chun Hsieh and Xi Liu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=AwWaBXLIJE}
}
```
