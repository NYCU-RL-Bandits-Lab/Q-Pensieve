# Q-Pensieve: Boosting Sample Efficiency of Multi-Objective RL Through Memory Sharing of Q-Snapshots

![](https://i.imgur.com/7Zuv6Jw.png)

**Q-Pensieve: Boosting Sample Efficiency of Multi-Objective RL Through Memory Sharing of Q-Snapshots**

Wei Hung\*, Bo Kai Huang\*, [Ping-Chun Hsieh](https://pinghsieh.github.io/), Xi Liu

*The Eleventh Conference on International Conference on Learning Representations (ICLR 2023)*

[[Paper]](https://openreview.net/pdf?id=AwWaBXLIJE)

## Code Structure

```
Q-Pensieve/
 ├── environments/ --- the test environment includes continuous Deep Sea Treasure (DST), multi-objective continuous LunarLander and multi-objective MuJoCo
 ├── 3pref_table.npy 4pref_table.npy 5pref_table.npy  --- testing preference generated by leveraging the Dirichlet distribution
 ├── agent.py --- the trainning agent of Q-Pensieve
 ├── test.py ---  main execution file for testing model with Q-Pensieve algorithms
 ├── compute_hv.py --- the execution file to caculate the hypervolume with trained model
 ├── main.py ---  main execution file for Q-Pensieve algorithms
 ├── model.py --- the model of Q-Pensieve
 ├── base.py --- the structure of Q replay buffer and trajectory replay buffer
 └── utils.py --- utility functions
 
 

```

## Requirements
- Python 3.7.9
- Pytorch 1.3.1
```
pip install -r requirements.txt
```
## Examples
### Training
You can directly use the following command to train.
```shell
python main.py --seed 1 --prefer 4 --buf_num 4
```
You can also edit the hyperparamter in configs.
### Testing
```
python test.py --prefer 4 --buf_num 4
```

## Refference
[rltorch](https://github.com/toshikwa/rltorch) -  a simple framework for reinforcement learning in PyTorch.
